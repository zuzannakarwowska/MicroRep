{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d99809-b908-4013-ab42-095351673917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from skbio.stats.ordination import pcoa    \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from torchmetrics.regression import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "import pandas as pd\n",
    "import skbio\n",
    "from skbio import TreeNode\n",
    "from io import StringIO\n",
    "from ete3 import Tree\n",
    "from skbio import diversity \n",
    "from skbio.diversity.beta import unweighted_unifrac, weighted_unifrac\n",
    "from scipy.spatial.distance import braycurtis\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from io import StringIO\n",
    "from ete3 import Tree\n",
    "import phylodm\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import train, tune\n",
    "from ray.tune import ResultGrid\n",
    "from scipy.spatial import procrustes\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "#https://medium.com/@hunter-j-phillips/the-embedding-layer-27d9c980d124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957435cb-d3fe-4cf0-b689-a0b3c3dc2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zkarwowska/TomaszLab Dropbox/Zuzanna Karwowsk/My Mac (zkarwowska’s MacBook Pro)/Desktop/microbiome_gpt\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca2c517-de2e-4ff2-b5d2-6e504ca95f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_csv('inputs/taxonomy.csv', index_col= [0], low_memory=False).fillna(0).sort_index()\n",
    "\n",
    "metadata = pd.read_csv('inputs/metadata.csv', index_col= [0], low_memory=False).sort_index()\n",
    "metadata = metadata[metadata['sample_id'].isin(taxonomy.index)]\n",
    "\n",
    "metadata['SICK'] = np.where(metadata.disease == 'healthy', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09438b0f-8bbc-4a60-9e42-4859ad583eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter low abundance bacteria\n",
    "\n",
    "def filter_prevalence(df, treshold = 0.5):\n",
    "    '''features as columns'''\n",
    "    df_binary = df.copy()\n",
    "    df_binary[df_binary>0]=1\n",
    "    df_binary_sum = df_binary.sum(axis=0)\n",
    "    \n",
    "    keep_features = df_binary_sum[df_binary_sum > df.shape[0]*treshold].index\n",
    "    filtered_df = df[keep_features]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def filter_on_abundance(df, abundance_treshold = 1e-2):\n",
    "    '''features as columns'''\n",
    "    df_relab = df.div(df.sum(axis=1), axis=0)\n",
    "    df_relab_mean = df_relab.mean()\n",
    "\n",
    "    keep_features = df_relab_mean[df_relab_mean > abundance_treshold].index\n",
    "    filtered_df = df[keep_features]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "filtered_taxonomy = filter_on_abundance(filter_prevalence(taxonomy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3cd7885-ee27-4d23-a78e-b9411a74a212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_taxonomy = filtered_taxonomy.sample(50)\n",
    "filtered_taxonomy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b606268-e1d9-4954-b6d3-44042f4e141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacteria_tensor torch.Size([10, 30, 1])\n",
      "bacteria_encoded torch.Size([30, 8])\n",
      "bacteria_transformed torch.Size([10, 30, 8])\n",
      "bacteria_encoded_expanded torch.Size([1, 30, 8])\n",
      "result torch.Size([10, 30, 8])\n",
      "output torch.Size([30, 10, 8])\n",
      "mean_vector torch.Size([10, 8])\n",
      "mean torch.Size([10, 8])\n",
      "z torch.Size([10, 8])\n",
      "D_vector torch.Size([10, 8])\n",
      "D_vector_repeated torch.Size([10, 30, 8])\n",
      "result torch.Size([10, 30, 8])\n",
      "torch.Size([30, 10, 8]) torch.Size([10, 30, 8])\n",
      "output torch.Size([30, 10, 8])\n",
      "output torch.Size([10, 30, 8])\n",
      "torch.Size([10, 30, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2017444102144.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 8\n",
    "num_bacteria = filtered_taxonomy.shape[1]\n",
    "batch_size = filtered_taxonomy.shape[0]\n",
    "\n",
    "bacteria_tensor = torch.tensor(filtered_taxonomy.values).float()\n",
    "\n",
    "# Krok 1: Zmieniamy kształt tensora na (B, #Bac, 1)\n",
    "bacteria_tensor = bacteria_tensor.unsqueeze(-1)\n",
    "print('bacteria_tensor', bacteria_tensor.shape)\n",
    "\n",
    "# Krok 2: Tworzenie \"bacteria encoding\" jako `(Bact, D)`\n",
    "bacteria_embedding = nn.Embedding(num_bacteria, embedding_dim)\n",
    "bacteria_indices = torch.arange(num_bacteria)\n",
    "bacteria_encoded = bacteria_embedding(bacteria_indices)\n",
    "print('bacteria_encoded', bacteria_encoded.shape)\n",
    "\n",
    "# Krok 3\n",
    "linear_layer = nn.Linear(1, embedding_dim)\n",
    "# Przekształcenie tensora (B, #Bac, 1) do (B, #Bac, D)\n",
    "bacteria_transformed = linear_layer(bacteria_tensor)\n",
    "print('bacteria_transformed', bacteria_transformed.shape)\n",
    "\n",
    "\n",
    "# Krok 4: Dodanie nowego wymiaru na początku, aby uzyskać `(1, #Bac, D)`\n",
    "bacteria_encoded_expanded = bacteria_encoded.unsqueeze(0)\n",
    "print('bacteria_encoded_expanded', bacteria_encoded_expanded.shape)\n",
    "\n",
    "# Dodaj krok 4 do sekwencji z 3\n",
    "bacteria_encoded_broadcasted = bacteria_encoded_expanded.expand(bacteria_transformed.shape)\n",
    "result = bacteria_encoded_broadcasted + bacteria_transformed\n",
    "print('result', result.shape)\n",
    "\n",
    "# Krok 5: wrzuć do transformera\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "\n",
    "# Transponowanie result do kształtu (sequence_length, batch_size, embedding_dim)\n",
    "# Ponieważ Transformer oczekuje danych w formacie (S, N, E)\n",
    "result_transposed = result.permute(1, 0, 2)\n",
    "# Przejście przez 3 warstwy transformera\n",
    "output = transformer_encoder(result_transposed)\n",
    "print('output', output.shape)\n",
    "\n",
    "# Uśrednij\n",
    "mean_vector = output.mean(dim=0)\n",
    "print('mean_vector', mean_vector.shape)\n",
    "\n",
    "# Mean and Logvar\n",
    "mean_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "mu_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "# Obliczanie mean i mu\n",
    "mean = mean_layer(mean_vector)\n",
    "logvar = mu_layer(mean_vector)\n",
    "print('mean', mean.shape)\n",
    "\n",
    "# Reparametrize\n",
    "def reparameterize(mean, log_var):\n",
    "    batch, dim = mean.shape\n",
    "    epsilon = Normal(0, 1).sample((batch, dim)).to(mean.device)\n",
    "    return mean + torch.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "z = reparameterize(mean, logvar)\n",
    "print('z', z.shape)\n",
    "\n",
    "#Weź sekwencję z latentu i przy pomocy Linear przekształć ją do wektora D-wymiarowego\n",
    "linear_transform = nn.Linear(embedding_dim, embedding_dim)\n",
    "D_vector = linear_transform(z)\n",
    "print('D_vector', D_vector.shape)\n",
    "\n",
    "# Powtórz ten wektor #Bac razy dostając sekwencję wymiaru (B, #Bac, D)\n",
    "D_vector_expanded = D_vector.unsqueeze(1)\n",
    "D_vector_repeated = D_vector_expanded.repeat(1, num_bacteria, 1)\n",
    "print('D_vector_repeated', D_vector_repeated.shape)\n",
    "\n",
    "#Dodaj “bacteria encoding”\n",
    "result = D_vector_repeated + bacteria_encoded\n",
    "print('result', result.shape)\n",
    "\n",
    "# Użycie 3 warstw transformera (decoder)\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=4)\n",
    "\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "\n",
    "memory = torch.randn(batch_size, num_bacteria, embedding_dim)\n",
    "memory = memory.permute(1, 0, 2)\n",
    "result_with_encoding = result.permute(1, 0, 2)\n",
    "\n",
    "print(memory.shape, result.shape)\n",
    "\n",
    "output = transformer_decoder(result_with_encoding, memory)\n",
    "print(\"output\", output.shape)\n",
    "\n",
    "output = output.permute(1, 0, 2)\n",
    "print(\"output\", output.shape)\n",
    "\n",
    "linear_transform = nn.Linear(embedding_dim, 1)\n",
    "output_transformed = linear_transform(output)\n",
    "\n",
    "print(output_transformed.shape)\n",
    "\n",
    "true_values = bacteria_tensor\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(linear_transform.parameters(), lr=0.001)\n",
    "loss = criterion(output_transformed, true_values)\n",
    "\n",
    "# Backpropagation i optymalizacja\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4775c291-ddd7-4ead-82ac-ac8397f1a499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 30, 1]), torch.Size([10, 30, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values.shape, output_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "263b2917-c94d-416c-aa6d-36f8bc174432",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = true_values.reshape((true_values.shape[0], true_values.shape[1]))\n",
    "ypred = output_transformed.reshape((output_transformed.shape[0], output_transformed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "022cf825-089c-49c4-9000-a04047b0fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 229247., 2239335., 4142136.,  319497., 3837933.,  268772.,  137676.,\n",
       "        1711264.,  215320.,  660408., 1624201., 1260792., 1965534.,  814927.,\n",
       "         216326., 1865244.,  474209.,    9577.,   63730., 3128068.,  117583.,\n",
       "         554078.,       0.,  148992.,  737692.,   45610.,  577982.,  230069.,\n",
       "         351512.,  789446.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb7f33bb-fb70-49d6-a0d5-93379b33d9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0118, -0.4562, -0.1482, -0.2330, -0.9547, -0.8142, -0.6284, -0.8120,\n",
       "        -0.2323,  0.2082, -0.5217, -0.5674, -0.1278,  0.4950, -0.1186, -1.3166,\n",
       "        -0.0483, -0.8770, -0.3322,  0.3068, -0.0199,  0.6789, -0.2523, -0.8710,\n",
       "        -0.3243, -0.2478, -0.2636,  0.4446, -0.8622, -0.4481],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b10374-9a63-4cae-a56d-1470eb8cd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class BacteriaModel(nn.Module):\n",
    "    def __init__(self, num_bacteria, embedding_dim):\n",
    "        super(BacteriaModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Bacteria encoding\n",
    "        self.bacteria_embedding = nn.Embedding(num_bacteria, embedding_dim)\n",
    "        \n",
    "        # Linear layer to transform (B, #Bac, 1) to (B, #Bac, D)\n",
    "        self.linear_layer = nn.Linear(1, embedding_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "        \n",
    "        # Mean and Logvar layers\n",
    "        self.mean_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.mu_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Linear transformation for latent vector\n",
    "        self.linear_transform = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=3)\n",
    "        \n",
    "        # Final linear layer to transform (B, #Bac, D) to (B, #Bac, 1)\n",
    "        self.output_transform = nn.Linear(embedding_dim, 1)\n",
    "    \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        batch, dim = mean.shape\n",
    "        epsilon = Normal(0, 1).sample((batch, dim)).to(mean.device)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "    def forward(self, bacteria_tensor):\n",
    "        batch_size = bacteria_tensor.size(0)\n",
    "        num_bacteria = bacteria_tensor.size(1)\n",
    "        \n",
    "        # Step 1: Transform (B, #Bac, 1) to (B, #Bac, D)\n",
    "        bacteria_transformed = self.linear_layer(bacteria_tensor)\n",
    "        \n",
    "        # Step 2: Create bacteria encoding\n",
    "        bacteria_indices = torch.arange(num_bacteria).to(bacteria_tensor.device)\n",
    "        bacteria_encoded = self.bacteria_embedding(bacteria_indices)\n",
    "        \n",
    "        # Expand bacteria encoding to match batch size and add to bacteria_transformed\n",
    "        bacteria_encoded_expanded = bacteria_encoded.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        result = bacteria_transformed + bacteria_encoded_expanded\n",
    "        \n",
    "        # Step 3: Pass through Transformer encoder\n",
    "        result_transposed = result.permute(1, 0, 2)  # (S, N, E)\n",
    "        output = self.transformer_encoder(result_transposed)\n",
    "        \n",
    "        # Step 4: Average over sequence (mean along the sequence dimension)\n",
    "        mean_vector = output.mean(dim=0)\n",
    "        \n",
    "        # Step 5: Compute mean and logvar\n",
    "        mean = self.mean_layer(mean_vector)\n",
    "        logvar = self.mu_layer(mean_vector)\n",
    "        \n",
    "        # Step 6: Reparameterize to get latent vector z\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # Step 7: Transform z to (B, D)\n",
    "        D_vector = self.linear_transform(z)\n",
    "        \n",
    "        # Step 8: Repeat D_vector for each bacterium to match (B, #Bac, D)\n",
    "        D_vector_expanded = D_vector.unsqueeze(1).repeat(1, num_bacteria, 1)\n",
    "        \n",
    "        # Add bacteria encoding again\n",
    "        result_with_encoding = D_vector_expanded + bacteria_encoded_expanded\n",
    "        \n",
    "        # Step 9: Pass through Transformer decoder\n",
    "        memory = torch.randn(batch_size, num_bacteria, self.embedding_dim).to(bacteria_tensor.device)\n",
    "        memory = memory.permute(1, 0, 2)  # (S, N, E)\n",
    "        result_with_encoding = result_with_encoding.permute(1, 0, 2)  # (S, N, E)\n",
    "        output = self.transformer_decoder(result_with_encoding, memory)\n",
    "        \n",
    "        # Step 10: Transform output to (B, #Bac, 1)\n",
    "        output = output.permute(1, 0, 2)  # Back to (B, #Bac, D)\n",
    "        output_transformed = self.output_transform(output)\n",
    "        \n",
    "        return output_transformed, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3c72a-2ee9-44e6-bcff-0647c90f3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "filtered_taxonomy = filtered_taxonomy.div(filtered_taxonomy.sum(axis=1), axis=0)\n",
    "embedding_dim = 8\n",
    "num_bacteria = filtered_taxonomy.shape[1]  # replace with filtered_taxonomy.shape[1]\n",
    "batch_size = filtered_taxonomy.shape[0]  # replace with filtered_taxonomy.shape[0]\n",
    "\n",
    "model = BacteriaModel(num_bacteria, embedding_dim)\n",
    "bacteria_tensor = torch.tensor(filtered_taxonomy.values).float()\n",
    "bacteria_tensor = bacteria_tensor.unsqueeze(-1)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_transformed, mean, logvar = model(bacteria_tensor)\n",
    "    \n",
    "    loss = criterion(output_transformed, bacteria_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c05ae513-77a5-486b-b4ad-ea3be9bc95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = bacteria_tensor.reshape((bacteria_tensor.shape[0], bacteria_tensor.shape[1]))\n",
    "ypred = output_transformed.reshape((output_transformed.shape[0], output_transformed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "095e129e-1cd1-47a1-8f3e-341e3c01461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0080, 0.0779, 0.1441, 0.0111, 0.1336, 0.0094, 0.0048, 0.0595, 0.0075,\n",
       "        0.0230, 0.0565, 0.0439, 0.0684, 0.0284, 0.0075, 0.0649, 0.0165, 0.0003,\n",
       "        0.0022, 0.1089, 0.0041, 0.0193, 0.0000, 0.0052, 0.0257, 0.0016, 0.0201,\n",
       "        0.0080, 0.0122, 0.0275])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8933f954-7c02-47b8-af28-f8bc805275ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2368,  0.3632,  0.3669,  0.1195,  0.6309,  0.3628,  0.2577, -0.1238,\n",
       "         0.5537,  0.2589,  0.3329,  0.3159,  0.6331,  0.4046,  0.3246, -0.0170,\n",
       "         0.2125,  0.2085,  0.2566,  0.5124,  0.5993,  0.1942,  0.3124,  0.5302,\n",
       "         0.2581,  0.2395,  0.4017,  0.4363,  0.4361,  0.4310],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
